{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nimageran/cv/blob/main/test_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "sgZMw4KJqFLD",
        "outputId": "bf77fd72-6ea4-4012-e2bc-e9830d3c4eea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action End Test, Points: 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "train(model)\n",
        "test(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense\n",
        "#from keras.optimizers import sgd\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "\n",
        "class Catch(object):\n",
        "    def __init__(self, grid_size=10):\n",
        "        self.grid_size = grid_size\n",
        "        self.reset()\n",
        "\n",
        "    def _update_state(self, action):\n",
        "        \"\"\"\n",
        "        Input: action and states\n",
        "        Ouput: new states and reward\n",
        "        \"\"\"\n",
        "        state = self.state\n",
        "        if action == 0:  # left\n",
        "            action = -1\n",
        "        elif action == 1:  # stay\n",
        "            action = 0\n",
        "        else:\n",
        "            action = 1  # right\n",
        "        f0, f1, basket = state[0]\n",
        "        new_basket = min(max(1, basket + action), self.grid_size-1)\n",
        "        f0 += 1\n",
        "        out = np.asarray([f0, f1, new_basket])\n",
        "        out = out[np.newaxis]\n",
        "\n",
        "        assert len(out.shape) == 2\n",
        "        self.state = out\n",
        "\n",
        "    def _draw_state(self):\n",
        "        im_size = (self.grid_size,)*2\n",
        "        state = self.state[0]\n",
        "        canvas = np.zeros(im_size)\n",
        "        canvas[state[0], state[1]] = 1  # draw fruit\n",
        "        canvas[-1, state[2]-1:state[2] + 2] = 1  # draw basket\n",
        "        return canvas\n",
        "\n",
        "    def _get_reward(self):\n",
        "        fruit_row, fruit_col, basket = self.state[0]\n",
        "        if fruit_row == self.grid_size-1:\n",
        "            if abs(fruit_col - basket) <= 1:\n",
        "                return 1\n",
        "            else:\n",
        "                return -1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def _is_over(self):\n",
        "        if self.state[0, 0] == self.grid_size-1:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def observe(self):\n",
        "        canvas = self._draw_state()\n",
        "        return canvas.reshape((1, -1))\n",
        "\n",
        "    def act(self, action):\n",
        "        self._update_state(action)\n",
        "        reward = self._get_reward()\n",
        "        game_over = self._is_over()\n",
        "        return self.observe(), reward, game_over\n",
        "\n",
        "    def reset(self):\n",
        "        n = np.random.randint(0, self.grid_size-1, size=1)\n",
        "        m = np.random.randint(1, self.grid_size-2, size=1)\n",
        "        #self.state = np.asarray([0, n, m])[np.newaxis]\n",
        "        self.state = np.array([[0, n[0], m[0]]])\n",
        "\n",
        "class ExperienceReplay(object):\n",
        "    def __init__(self, max_memory=100, discount=.9):\n",
        "        self.max_memory = max_memory\n",
        "        self.memory = list()\n",
        "        self.discount = discount\n",
        "\n",
        "    def remember(self, states, game_over):\n",
        "        # memory[i] = [[state_t, action_t, reward_t, state_t+1], game_over?]\n",
        "        self.memory.append([states, game_over])\n",
        "        if len(self.memory) > self.max_memory:\n",
        "            del self.memory[0]\n",
        "\n",
        "    def get_batch(self, model, batch_size=10):\n",
        "        len_memory = len(self.memory)\n",
        "        num_actions = model.output_shape[-1]\n",
        "        env_dim = self.memory[0][0][0].shape[1]\n",
        "        inputs = np.zeros((min(len_memory, batch_size), env_dim))\n",
        "        targets = np.zeros((inputs.shape[0], num_actions))\n",
        "        for i, idx in enumerate(np.random.randint(0, len_memory,\n",
        "                                                  size=inputs.shape[0])):\n",
        "            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
        "            game_over = self.memory[idx][1]\n",
        "\n",
        "            inputs[i:i+1] = state_t\n",
        "            # There should be no target values for actions not taken.\n",
        "            # Thou shalt not correct actions not taken #deep\n",
        "            targets[i] = model.predict(state_t)[0]\n",
        "            Q_sa = np.max(model.predict(state_tp1)[0])\n",
        "            if game_over:  # if game_over is True\n",
        "                targets[i, action_t] = reward_t\n",
        "            else:\n",
        "                # reward_t + gamma * max_a' Q(s', a')\n",
        "                targets[i, action_t] = reward_t + self.discount * Q_sa\n",
        "        return inputs, targets\n",
        "\n",
        "\n",
        "# parameters\n",
        "epsilon = .1  # exploration\n",
        "num_actions = 3  # [move_left, stay, move_right]\n",
        "epoch = 1000\n",
        "max_memory = 500\n",
        "hidden_size = 100\n",
        "batch_size = 1\n",
        "grid_size = 10\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(hidden_size, input_shape=(grid_size**2,), activation='relu'))\n",
        "model.add(Dense(hidden_size, activation='relu'))\n",
        "model.add(Dense(num_actions))\n",
        "model.compile(SGD(lr=.2), \"mse\")\n",
        "\n",
        "# If you want to continue training from a previous model, just uncomment the line bellow\n",
        "# model.load_weights(\"model.h5\")\n",
        "\n",
        "# Define environment/game\n",
        "env = Catch(grid_size)\n",
        "\n",
        "# Initialize experience replay object\n",
        "exp_replay = ExperienceReplay(max_memory=max_memory)\n",
        "\n",
        "def train(model):\n",
        "    # Train\n",
        "    win_cnt = 0\n",
        "    for e in range(1):\n",
        "        loss = 0.\n",
        "        env.reset()\n",
        "        game_over = False\n",
        "        # get initial input\n",
        "        input_t = env.observe()\n",
        "\n",
        "        while not game_over:\n",
        "            input_tm1 = input_t\n",
        "            # get next action\n",
        "            if np.random.rand() <= epsilon:\n",
        "                action = np.random.randint(0, num_actions, size=1)\n",
        "            else:\n",
        "                q = model.predict(input_tm1)\n",
        "                action = np.argmax(q[0])\n",
        "\n",
        "            # apply action, get rewards and new state\n",
        "            input_t, reward, game_over = env.act(action)\n",
        "            if reward == 1:\n",
        "                win_cnt += 1\n",
        "\n",
        "            # store experience\n",
        "            exp_replay.remember([input_tm1, action, reward, input_t], game_over)\n",
        "\n",
        "            # adapt model\n",
        "            inputs, targets = exp_replay.get_batch(model, batch_size=batch_size)\n",
        "\n",
        "            display_screen(action,3000,inputs[0])\n",
        "\n",
        "            #loss += model.train_on_batch(inputs, targets)[0]\n",
        "            loss_value = model.train_on_batch(inputs, targets)\n",
        "            loss += loss_value\n",
        "        print(\"Epoch {:03d}/999 | Loss {:.4f} | Win count {}\".format(e, loss, win_cnt))"
      ],
      "metadata": {
        "id": "NIXf-uukynIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAlAcjEGqFLJ"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import seaborn\n",
        "seaborn.set()\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "from keras.models import model_from_json\n",
        "from qlearn import Catch\n",
        "from PIL import Image\n",
        "from IPython import display\n",
        "last_frame_time = 0\n",
        "translate_action = [\"Left\",\"Stay\",\"Right\",\"Create Ball\",\"End Test\"]\n",
        "grid_size = 10\n",
        "\n",
        "def display_screen(action, points, input_t):\n",
        "    global last_frame_time\n",
        "    display.clear_output(wait=True)\n",
        "\n",
        "    if isinstance(action, np.ndarray):\n",
        "        action = action.item()  # Convert array to scalar\n",
        "\n",
        "    print(\"Action %s, Points: %d\" % (translate_action[action], points))\n",
        "\n",
        "    if \"End\" not in translate_action[action]:\n",
        "        plt.imshow(input_t.reshape((grid_size,)*2),\n",
        "               interpolation='none', cmap='gray')\n",
        "        display.display(plt.gcf())\n",
        "    last_frame_time = set_max_fps(last_frame_time)\n",
        "def set_max_fps(last_frame_time,FPS = 1):\n",
        "    current_milli_time = lambda: int(round(time.time() * 1000))\n",
        "    sleep_time = 1./FPS - (current_milli_time() - last_frame_time)\n",
        "    if sleep_time > 0:\n",
        "        time.sleep(sleep_time)\n",
        "    return current_milli_time()\n",
        "def test(model):\n",
        "    global last_frame_time\n",
        "    plt.ion()\n",
        "    # Define environment, game\n",
        "    env = Catch(grid_size)\n",
        "    c = 0\n",
        "    last_frame_time = 0\n",
        "    points = 0\n",
        "    for e in range(10):\n",
        "        loss = 0.\n",
        "        env.reset()\n",
        "        game_over = False\n",
        "        # get initial input\n",
        "        input_t = env.observe()\n",
        "        display_screen(3, points, input_t)  # Starting with \"Create Ball\" action\n",
        "        c += 1\n",
        "        while not game_over:\n",
        "            input_tm1 = input_t\n",
        "            # get next action\n",
        "            q = model.predict(input_tm1)\n",
        "            action = np.argmax(q[0])\n",
        "            # apply action, get rewards and new state\n",
        "            input_t, reward, game_over = env.act(action)\n",
        "            points += reward\n",
        "            display_screen(action, points, input_t)  # Pass action and points here\n",
        "            c += 1\n",
        "    display_screen(4, points, input_t)  # Ending with \"End Test\" action\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}